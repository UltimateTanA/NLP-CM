# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xX8V_db4JfQ8zcHIuRcgkiCFIQsnUFua
"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Load dataset
file_path = '/content/SPAM text message 20170820 - Data.csv'
data = pd.read_csv(file_path)

# Display sample records and columns
sample_data = data.head()
columns = data.columns.tolist()

# Split dataset into train (70%), validation (15%), and test (15%)
train_val, test = train_test_split(data, test_size=0.15, stratify=data['Category'], random_state=42)
train, val = train_test_split(train_val, test_size=0.176, stratify=train_val['Category'], random_state=42)  # 0.176 * 0.85 ~= 15%

# Show counts in each split
train_count = train.shape[0]
val_count = val.shape[0]
test_count = test.shape[0]

sample_data, columns, train_count, val_count, test_count, train['Category'].value_counts(), val['Category'].value_counts(), test['Category'].value_counts()

from sklearn.model_selection import train_test_split

# First, split into train+temp (85%) and test (15%)
train_temp, test = train_test_split(
    data,
    test_size=0.15,
    random_state=42,    # makes the split reproducible
    stratify=data["Category"]   # keeps spam/ham ratio the same in all sets
)

# Next, split train_temp into train (70%) and validation (15%)
train, val = train_test_split(
    train_temp,
    test_size=0.1765,   # 0.1765 x 85% â‰ˆ 15%
    random_state=42,
    stratify=train_temp["Category"]
)

# Optional: check the sizes
print(f"Train size: {len(train)}")
print(f"Validation size: {len(val)}")
print(f"Test size: {len(test)}")

# Assuming you have already run the splitting code and have variables: train, val, test

# Save each split to a separate CSV file
train.to_csv("sms_train.csv", index=False)
val.to_csv("sms_val.csv", index=False)
test.to_csv("sms_test.csv", index=False)

print("Splits saved as sms_train.csv, sms_val.csv, and sms_test.csv")