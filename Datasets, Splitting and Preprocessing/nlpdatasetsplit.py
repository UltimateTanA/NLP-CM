# -*- coding: utf-8 -*-
"""NLPdatasetSplit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xX8V_db4JfQ8zcHIuRcgkiCFIQsnUFua
"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Load dataset
file_path = '/content/SPAM text message 20170820 - Data.csv'
data = pd.read_csv(file_path)

# Show first 5 rows
print("Sample records:\n", data.head(), "\n")

# Show column names and datatypes
print("Columns and data types:\n", data.dtypes, "\n")

# Show the number of rows and columns
print(f"Total data shape: {data.shape}\n")

# Show class distribution and percentage
print("Label counts:\n", data['Category'].value_counts(), "\n")
print("Label proportions (%):\n", round(data['Category'].value_counts(normalize=True)*100, 2), "\n")

# Show unique categories
print("Unique categories in 'Category' column:", data['Category'].unique(), "\n")

# Show missing value counts
print("Missing values in each column:\n", data.isnull().sum(), "\n")

# Show some example messages for each label
for cat in data['Category'].unique():
    print(f"Sample '{cat}' messages:\n", data[data['Category']==cat]['Message'].sample(3, random_state=42).to_list(), "\n")

# Split dataset into train (70%), validation (15%), and test (15%)
train_val, test = train_test_split(
    data, test_size=0.15, stratify=data['Category'], random_state=42
)
train, val = train_test_split(
    train_val, test_size=0.176, stratify=train_val['Category'], random_state=42
)

# Show counts in each split
print(f"Train size: {train.shape[0]}")
print(f"Validation size: {val.shape[0]}")
print(f"Test size: {test.shape[0]}\n")

# Show class distributions in each subset
print("Train label counts:\n", train['Category'].value_counts(), "\n")
print("Validation label counts:\n", val['Category'].value_counts(), "\n")
print("Test label counts:\n", test['Category'].value_counts(), "\n")

# Show average message length per split
print("Average message length in train:", train['Message'].apply(len).mean())
print("Average message length in val:", val['Message'].apply(len).mean())
print("Average message length in test:", test['Message'].apply(len).mean(), "\n")

from sklearn.model_selection import train_test_split

# First, split into train+temp (85%) and test (15%)
train_temp, test = train_test_split(
    data,
    test_size=0.15,
    random_state=42,    # makes the split reproducible
    stratify=data["Category"]   # keeps spam/ham ratio the same in all sets
)

# Next, split train_temp into train (70%) and validation (15%)
train, val = train_test_split(
    train_temp,
    test_size=0.1765,   # 0.1765 x 85% â‰ˆ 15%
    random_state=42,
    stratify=train_temp["Category"]
)

# Optional: check the sizes
print(f"Train size: {len(train)}")
print(f"Validation size: {len(val)}")
print(f"Test size: {len(test)}")

# Assuming you have already run the splitting code and have variables: train, val, test

# Save each split to a separate CSV file
train.to_csv("sms_train.csv", index=False)
val.to_csv("sms_val.csv", index=False)
test.to_csv("sms_test.csv", index=False)

print("Splits saved as sms_train.csv, sms_val.csv, and sms_test.csv")